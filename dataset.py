from itertools import filterfalse
from math import floor

import numpy as np
from numpy.lib import stride_tricks
import pandas as pd
from more_itertools import powerset

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.base import clone
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, FunctionTransformer

from torch.utils.data import Dataset, Subset, DataLoader

import utils
from utils import Scaler
from constants import DataDict, AssetID, DATASET_SYMBOLS, DUMMY_SCALER


class TimeSeriesDataset:
    def __init__(self, X, y,
                 period=100,
                 test_size=0.2,
                 scaler: Scaler = DUMMY_SCALER,
                 fit=True,
                 flatten=True,
                 name=None,
                 clone_scaler=False):
        """
        Dataset class for time series data (note: assumes data is already aligned)
        :param X: input data
        :param y: target data
        :param period: period of analysis
        :param test_size: proportion of dataset to use in the testing set
        :param scaler: scaler to use with the data. Doesn't scale data if not provided (default)
        :param flatten: whether to flatten the sliding windows generated by the dataset
        :param name: name of the dataset
        :param clone_scaler: whether to clone the scaler provided (prevents aliasing when building datasets in loops)
        """
        self.name = name  # set the dataset name
        # set the data index
        if isinstance(y, (pd.DataFrame, pd.Series)):
            self.index = y.index  # This may be useful for aligning time series plots

        # if desired, clone scaler to prevent aliasing (useful for building datasets with loops or list comprehensions)
        if clone_scaler:
            scaler = clone(scaler)

        self.scaler = scaler  # set the scaler for the dataset
        self.y = y[period - 1:]  # drop unusable values for y and set y

        # get the indices of end of the training set / start of the testing set
        train_end = test_start = floor(len(X) * (1 - test_size))

        if fit:
            self.scaler.fit(X[:train_end])  # fit the scaler to the training data
        X = self.scaler.transform(X)  # scale the data

        if isinstance(X, (pd.DataFrame, pd.Series)):  # no scaler is provided. Convert to numpy if DataFrame or Series
            X = X.to_numpy()

        features = X.shape[1]  # get the number of features

        # https://stackoverflow.com/questions/43185589/sliding-windows-from-2d-array-that-slides-along-axis-0-or-rows-to-give-a-3d-arra
        # Generate sliding windows
        nd0 = X.shape[0] - period + 1
        s0, s1 = X.strides
        self.X = stride_tricks.as_strided(X, shape=(nd0, period, features), strides=(s0, s0, s1))

        if flatten:  # flatten the sliding windows
            self.X = self.X.reshape(-1, period * features)

        # set training and testing subsets
        self.X_train = self.X[:train_end]
        self.y_train = self.y[:train_end]
        self.X_test = self.X[test_start:]
        self.y_test = self.y[test_start:]


class MultiAssetDataset(TimeSeriesDataset):
    def __init__(self, symbols: list[AssetID], data: DataDict, y, **kwargs):
        """
        Dataset class for time series datasets with multiple assets
        :param symbols: list of tuples (asset type, SYMBOL)
        :param data: dictionary of data to pull data from based on symbols
        :param y: target data
        :param kwargs: arbitrary keyword arguments to pass to TimeSeriesDataset
        """
        self.symbols = symbols
        self.dfs = [utils.get_df_from_symbol(asset_type, symbol, data) for asset_type, symbol in self.symbols]

        joined = utils.join_datasets(self.dfs)
        X, y = utils.align_data(joined, y)

        super().__init__(X, y, **kwargs)


class TorchTimeSeriesDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray, period=10):
        """
        :param X: m by n input matrix. m=number of time series, n=total data points
        :param y: n label vector
        :param period: length of time series to train on
        """
        self.X = X
        self.y = y
        self.period = period

    def __len__(self):
        m = self.X.size(0)
        return m - self.period

    def __getitem__(self, index):
        x = self.X[index:index + self.period]
        y = self.y[index + self.period]
        return x, y


class TorchTimeSeriesDataLoader:
    def __init__(self, X, y, validation_split=0.20, test_split=0.20, batch_size=4, period=100):
        self.dataset = TorchTimeSeriesDataset(X, y, period=period)

        test_start = floor(len(self.dataset) * (1 - test_split))  # Starting index of testing set
        subset_split = validation_split * (1 - test_split)  # get correct validation split after pulling test_split

        train_indices, validation_indices = train_test_split(range(test_start), test_size=subset_split)
        test_indices = range(test_start, len(self.dataset))
        self.train_dataset = Subset(self.dataset, train_indices)
        self.validation_dataset = Subset(self.dataset, validation_indices)
        self.test_dataset = Subset(self.dataset, test_indices)

        self.train_data_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)
        self.validation_data_loader = DataLoader(self.validation_dataset, batch_size=batch_size, shuffle=True)
        self.test_data_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)
        self.all_data_loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=False)


def build_datasets(period=5, brn_features=5, zero_col_thresh=1, replace_zero=None, **pca_kwargs) -> list[TimeSeriesDataset]:
    """
    Builds the full suite of datasets for experimentation
    :param period: period for sliding windows
    :param brn_features: number of features to generate for brownian motion datasets
    :param zero_col_thresh: proportion of a column that must be zero to drop it (passed to make_percent_dict)
    :param replace_zero: value to replace zeros in y_base with. No replacement if None (default)
    :param pca_kwargs: arbitrary keyword arguments to pass to the PCA initializer
    :return: list of datasets
    """
    # Load all the data
    raw_data = utils.load_data()
    percent_data = utils.make_percent_dict(raw_data, zero_col_thresh=zero_col_thresh)

    # Generate the FULL available y set
    y_base = utils.make_percent_series(raw_data['stock']['SPY.US']['close'])
    y_base = y_base.apply(np.sign).shift(-1).iloc[:-1]
    if replace_zero is not None:  # replace zeros if desired
        y_base = y_base.replace(0, replace_zero)  # replace 0s with -1 so classification is binary

    scalers = {
        '': StandardScaler(),
        'PCA': make_pipeline(StandardScaler(),
                             PCA(**pca_kwargs)),
        'Fourier': make_pipeline(StandardScaler(),
                                 FunctionTransformer(utils.fourier, utils.inverse_fourier)),
        'PCA into Fourier': make_pipeline(StandardScaler(),
                                          PCA(**pca_kwargs),
                                          FunctionTransformer(utils.fourier, utils.inverse_fourier)),
        'Fourier into PCA': make_pipeline(StandardScaler(),
                                          FunctionTransformer(utils.fourier, utils.inverse_fourier),
                                          PCA(**pca_kwargs))
    }

    # Simple dataset generation
    brn_X = utils.generate_brownian_motion(len(y_base), brn_features, cumulative=True)
    norm_X = utils.generate_brownian_motion(len(y_base), brn_features)
    simple_data = {
        'SPY Raw': utils.align_data(raw_data['stock']['SPY.US'], y_base),
        'SPY %': utils.align_data(percent_data['stock']['SPY.US'], y_base)
    }

    datasets = [
        TimeSeriesDataset(brn_X, y_base, period=period, scaler=StandardScaler(), name='Brownian Motion'),
        TimeSeriesDataset(norm_X, y_base, period=period, scaler=StandardScaler(), name='Normal Sample'),
    ]

    datasets.extend([
        TimeSeriesDataset(X, y,
                          period=period,
                          scaler=scaler,
                          name=f'{data_name} {scaler_name}'.rstrip(),
                          clone_scaler=True)
        for data_name, (X, y) in simple_data.items()
        for scaler_name, scaler in scalers.items()
    ])

    # MULTI-ASSET DATASET GENERATION
    # Generate powerset of desired available asset_types
    asset_powerset = filterfalse(lambda x: x == (), powerset(DATASET_SYMBOLS.keys()))

    for asset_set in asset_powerset:  # for each set of assets in the powerset of asset types
        symbol_list = utils.generate_symbol_list(asset_set)  # get list of symbols for those assets

        for percent in [False, True]:  # raw data vs. percent-change
            data = percent_data if percent else raw_data  # set desired dataset
            for scaler_name, scaler in scalers.items():  # Select desired scaler
                postfix = f"{'%' if percent else 'Raw'} {scaler_name}".rstrip()  # dataset name postfix

                # generate and append new MultiAssetDataset to dataset list
                datasets.append(MultiAssetDataset([('stock', 'SPY.US')] + symbol_list[0], data, y_base,
                                                  name=f'{symbol_list[1]} {postfix}', period=period, scaler=scaler,
                                                  clone_scaler=True))

    return datasets
